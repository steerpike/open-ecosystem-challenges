{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Open Ecosystem Challenges","text":"<p>Welcome to Open Ecosystem Challenges! \ud83d\ude80</p> <p>These are hands-on, recurring prompts designed to help you practice Cloud Native, OpenTelemetry, AI/ML, and other open source skills.</p> <p>Each challenge runs in a pre-provisioned environment, so you can focus on solving real problems, not setup headaches.</p> <p>What makes these challenges special:</p> <ul> <li>\ud83c\udfaf Skill-focused - Target specific technologies with clear objectives</li> <li>\ud83d\udcd6 Story-driven - Learn through engaging narratives</li> <li>\ud83d\ude80 Zero setup - Run in GitHub Codespaces, pre-configured and ready</li> <li>\u2705 Two-step verification - Smoke tests and GitHub Actions validate your solution</li> <li>\ud83c\udf93 Three levels - Beginner, Intermediate, and Expert for each adventure</li> </ul>"},{"location":"#available-adventures","title":"\ud83d\uddfa\ufe0f Available Adventures","text":"<p>Browse the available adventures and pick one that interests you:</p>"},{"location":"#echoes-lost-in-orbit","title":"Echoes Lost in Orbit","text":"<p>Story: Restore interstellar communications by fixing broken GitOps setups, progressive delivery systems, and observability pipelines across three galactic missions.</p> Level Name \ud83e\udde0 Key Learnings \ud83d\udfe2 Beginner Broken Echoes <ul><li>Debug GitOps flows with Argo CD</li><li>ApplicationSet templating &amp; pitfalls</li><li>Environment isolation &amp; namespaces</li><li>Sync policies: automated, prune &amp; self-heal</li></ul> \ud83d\udfe1 Intermediate The Silent Canary <ul><li>Progressive delivery with Argo Rollouts</li><li>Canary deployments &amp; automated analysis</li><li>Write PromQL queries for health validation</li><li>Kube-state-metrics for deployment decisions</li></ul> \ud83d\udd34 Expert Hyperspace Operations &amp; Transport <ul><li>Configure OpenTelemetry Collector pipelines</li><li>Spanmetrics connector (traces \u2192 metrics)</li><li>Detect \"idle canaries\" with traffic validation</li><li>Distributed tracing with Jaeger</li><li>Trace-derived metrics for progressive delivery</li></ul> <p>More adventures coming soon!</p>"},{"location":"#how-it-works","title":"\ud83c\udfae How It Works","text":"<p>Each level is independent - start anywhere, complete in any order. Levels share a connected story but have their own:</p> <ul> <li>Codespace configuration</li> <li>Documentation and guides</li> <li>Validation tests</li> </ul> <p>Levels:</p> <ul> <li>\ud83d\udfe2 Beginner: New to the technology? Start here to learn the basics</li> <li>\ud83d\udfe1 Intermediate: Comfortable with fundamentals? Practice advanced patterns</li> <li>\ud83d\udd34 Expert: Want a real challenge? Tackle complex real-world scenarios</li> </ul>"},{"location":"#how-to-verify-your-solution","title":"\u2705 How to Verify Your Solution","text":"<p>Each challenge includes a two-step verification process:</p> <ol> <li>Smoke Test - Run locally in your Codespace for quick validation</li> <li>GitHub Actions Workflow - Comprehensive verification you manually trigger after pushing</li> </ol> <p>\ud83d\udcd6 Learn more: Read the complete Verification Guide for detailed instructions on both steps.</p>"},{"location":"#faq","title":"\u2753 FAQ","text":"<p>Do I need to complete levels in order? No! Each level is independent. Start wherever you feel comfortable.</p> <p>Can I use these for team training? Absolutely! Perfect for upskilling, onboarding, internal training, and hackathons.</p> <p>Are there costs? GitHub Codespaces offers free hours per month - usually sufficient for individual use. Check GitHub's pricing for details.</p> <p>Need help? Check adventure-specific docs, open an issue, or start a discussion.</p>"},{"location":"#ready-to-start","title":"\ud83d\ude80 Ready to Start?","text":"<p>Choose your adventure and begin learning!</p>"},{"location":"start-a-challenge/","title":"Start a Challenge","text":"<p>The setup process is the same for all challenges. Fork the Open Ecosystem Challenges repository, start a Codespace with your challenge's configuration, and wait for the infrastructure to deploy.</p>"},{"location":"start-a-challenge/#1-fork-the-repository","title":"1. Fork the Repository","text":"<p>Click the Fork button in the top-right corner of the GitHub repo or use this link.</p>"},{"location":"start-a-challenge/#already-have-a-fork","title":"Already Have a Fork?","text":"<p>If you've completed a previous challenge and already have a fork, sync it first to get the latest updates:</p> <ol> <li>Go to your fork on GitHub</li> <li>Click Sync fork (above the file list)</li> <li>Click Update branch if changes are available</li> </ol> <p>This ensures you have the latest challenge content before starting a new level.</p>"},{"location":"start-a-challenge/#2-start-a-codespace","title":"2. Start a Codespace","text":"<ul> <li>From your fork, click the green Code button \u2192 Codespaces hamburger menu \u2192 New with options </li> <li>Select the configuration that matches your challenge (e.g., \"Adventure 01 | \ud83d\udfe2 Beginner (Broken Echoes)\" for the   beginner level of adventure 1)   </li> </ul> <p>\u26a0\ufe0f Important: The challenge will not work if you choose a configuration that does not match your challenge (or the default).</p>"},{"location":"start-a-challenge/#3-wait-for-infrastructure-to-deploy","title":"3. Wait for Infrastructure to Deploy","text":"<p>Your Codespace will automatically provision all necessary challenge infrastructure. This usually takes around 5-10 minutes.</p> <p>\ud83d\udca1 Tip: To check the progress press <code>Cmd + Shift + P</code> (or <code>Ctrl + Shift + P</code> on Windows/Linux) and search for <code>View Creation Log</code> (available after a few moments once the Codespace has initialized).</p> <p>Once complete, return to your specific challenge documentation for level-specific instructions on solving the challenge.</p>"},{"location":"verification/","title":"Verify Your Solution","text":"<p>Each challenge includes a three-step verification process to help you validate and share your solution:</p> <ol> <li>Local Smoke Test - Quick validation in your Codespace</li> <li>Full Verification Workflow - Comprehensive validation via GitHub Actions</li> <li>Submit Your Results - Share your success with the community</li> </ol> <p>This process is designed to give you confidence in your solution without directly revealing the answers.</p>"},{"location":"verification/#step-1-local-smoke-test","title":"\ud83e\uddea Step 1: Local Smoke Test","text":"<p>The smoke test is a script that runs directly in your Codespace to check basic success criteria.</p>"},{"location":"verification/#what-it-checks","title":"What It Checks","text":"<ul> <li>\u2705 Basic functionality (e.g., services are reachable)</li> <li>\u2705 Key resources are deployed correctly</li> <li>\u2705 Essential configuration is in place</li> </ul>"},{"location":"verification/#what-it-doesnt-check","title":"What It Doesn't Check","text":"<p>The smoke test deliberately avoids checking certain criteria to prevent revealing the solution. These more complex validations are performed by the full verification workflow.</p>"},{"location":"verification/#how-to-run","title":"How to Run","text":"<p>Each challenge level has its own smoke test script. Run it from the repository root:</p> <pre><code>adventures/&lt;adventure-name&gt;/&lt;level&gt;/smoke-test.sh\n</code></pre> <p>Example for Adventure 01, Beginner level:</p> <pre><code>adventures/01-echoes-lost-in-orbit/easy/smoke-test.sh\n</code></pre>"},{"location":"verification/#understanding-the-results","title":"Understanding the Results","text":"<p>\u2705 If the smoke test passes:</p> <ul> <li>Your solution likely meets all requirements</li> <li>Proceed to Step 2 for full verification</li> </ul> <p>\u274c If the smoke test fails:</p> <ul> <li>Review the error messages and hints provided</li> <li>Check your solution against the challenge objectives</li> <li>Make adjustments and run the test again</li> </ul>"},{"location":"verification/#step-2-full-verification-workflow","title":"\ud83d\udd04 Step 2: Full Verification Workflow","text":"<p>The full verification workflow runs on GitHub Actions and performs comprehensive validation of your solution.</p>"},{"location":"verification/#prerequisites","title":"Prerequisites","text":""},{"location":"verification/#enable-github-actions-in-your-fork","title":"Enable GitHub Actions in Your Fork","text":"<p>If this is a new fork, GitHub Actions workflows are disabled by default. You need to enable them:</p> <ol> <li>Go to your fork on GitHub</li> <li>Click the Actions tab</li> <li>Click the green button \"I understand my workflows, go ahead and enable them\"</li> </ol> <p>\ud83d\udca1 Note: This is a one-time setup for your fork. Once enabled, workflows will be available for all challenges.</p>"},{"location":"verification/#running-the-verification-workflow","title":"Running the Verification Workflow","text":"<p>The verification workflow validates more complex success criteria that cannot be checked locally without revealing the solution.</p>"},{"location":"verification/#when-to-run","title":"When to Run","text":"<p>Run the verification workflow after your smoke test passes.</p>"},{"location":"verification/#how-to-run_1","title":"How to Run","text":"<ol> <li> <p>Commit and push your changes to the <code>main</code> branch of your fork:    <pre><code>git add .\ngit commit -m \"Solved Adventure 01 - Easy level\"\ngit push origin main\n</code></pre></p> </li> <li> <p>Manually trigger the workflow on GitHub:</p> <ul> <li>Go to your fork on GitHub</li> <li>Click the Actions tab</li> <li>Select the \"Verify Adventure\" workflow from the left sidebar</li> <li>Click the \"Run workflow\" dropdown button</li> <li>Select the challenge you want to verify (e.g., <code>Adventure 01 | \ud83d\udfe2 Easy (Broken Echoes)</code>)</li> <li>Click \"Run workflow\"</li> </ul> </li> <li> <p>Wait for the workflow to complete</p> </li> </ol>"},{"location":"verification/#understanding-the-results_1","title":"Understanding the Results","text":"<p>\u2705 If the workflow passes:</p> <ul> <li>\ud83c\udf89 Congratulations! You've successfully completed the challenge!</li> <li>Proceed to Step 3 to claim your completion</li> </ul> <p>\u274c If the workflow fails:</p> <ul> <li>Click on the failed workflow run to see detailed logs</li> <li>Review what criteria were not met</li> <li>Adjust your solution and try again</li> <li>Don't hesitate to open a discussion if you're stuck</li> </ul>"},{"location":"verification/#step-3-submit-your-results","title":"\ud83d\udcf8 Step 3: Submit Your Results","text":"<p>Once your verification workflow passes, it's time to share your success with the community!</p>"},{"location":"verification/#how-to-submit","title":"How to Submit","text":"<ol> <li> <p>Take a screenshot of your successful workflow run on GitHub Actions</p> <ul> <li>The screenshot should show the green checkmark and \"Success\" status</li> <li>Include the workflow name and your GitHub username in the screenshot</li> </ul> </li> <li> <p>Post your screenshot as a comment to the original challenge thread</p> <ul> <li>Find the discussion thread for your specific adventure and level</li> <li>Add a comment with your screenshot</li> <li>Optionally, share any interesting learnings or challenges you faced \ud83d\ude4c</li> </ul> </li> <li> <p>Celebrate! \ud83c\udf89</p> <ul> <li>You've officially completed the challenge</li> <li>Your contribution is now part of the Open Ecosystem community</li> <li>Ready for more? Move on to the next level or choose another adventure!</li> </ul> </li> </ol>"},{"location":"verification/#why-submit","title":"Why Submit?","text":"<ul> <li>Recognition: Get acknowledged by the community for your achievement</li> <li>Inspiration: Help motivate others who are working on the same challenge</li> <li>Community: Connect with fellow learners and share insights</li> <li>Progress Tracking: Keep a record of your completed challenges</li> </ul>"},{"location":"verification/#tips-for-success","title":"\ud83c\udfaf Tips for Success","text":"<ul> <li>Read the challenge objectives carefully: They outline exactly what needs to be achieved</li> <li>Run the smoke test before committing: It provides fast feedback during development</li> <li>Check the workflow logs: They contain valuable debugging information if verification fails</li> <li>Don't give up: These challenges are designed to be... challenging! Learning happens through iteration</li> </ul>"},{"location":"verification/#need-help","title":"\ud83e\udd1d Need Help?","text":"<p>If you're stuck or have questions about verification:</p> <ul> <li>\ud83d\udcac Start a discussion</li> <li>\ud83d\udc1b Report an issue if you think something is   broken</li> <li>\ud83d\udcd6 Check the adventure-specific documentation for hints and resources</li> </ul>"},{"location":"verification/#why-this-verification-process","title":"\ud83d\udd12 Why This Verification Process?","text":"<p>The three-step approach balances learning, validation, and community engagement:</p> <ul> <li>Smoke tests give you fast, local feedback without an internet connection</li> <li>Workflow verification ensures comprehensive validation without giving away solutions in the local scripts</li> <li>Community submission celebrates your achievement and contributes to the learning ecosystem </li> </ul> <p>Together, they provide confidence that your solution is correct while preserving the learning experience</p> <p>Happy solving! \ud83d\ude80</p>"},{"location":"01-echoes-lost-in-orbit/","title":"\ud83d\udef0\ufe0f Adventure 01: Echoes Lost in Orbit","text":"<p>Welcome to the first challenge in the Open Ecosystem Challenge series! Your mission: restore interstellar communication by fixing a broken GitOps setup. This is a hands-on troubleshooting exercise using Kubernetes, Argo CD, and Kustomize.</p> <p>The entire infrastructure is pre-provisioned in your Codespace \u2014 Kubernetes cluster, Argo CD, and sample app are ready to go. You don\u2019t need to set up anything locally. Just focus on solving the problem.</p>"},{"location":"01-echoes-lost-in-orbit/#the-backstory","title":"\ud83e\ude90 The Backstory","text":"<p>Welcome aboard the GitOps Starliner, a multi-species engineering vessel orbiting the vibrant planet of Polaris-9. Life in this quadrant is wonderfully diverse \u2014 from the whispering cloud-dwellers of Nebulon to the rhythmic click-speakers of Crustacea Prime.</p> <p>Communication between species used to be seamless, thanks to the Echo Server, a universal translator that instantly echoed your words in the listener's native format.</p> <p>But lately, something's off.</p> <p>Messages are getting scrambled. Some transmissions never arrive. The Echo Server, deployed across the Staging Moonbase and the Production Outpost, is no longer syncing properly. The Argo CD dashboard shows no active deployments, and telemetry is suspiciously quiet.</p> <p>You've been assigned to restore interstellar communication before the next critical mission.</p>"},{"location":"01-echoes-lost-in-orbit/#choose-your-level","title":"\ud83c\udfae Choose Your Level","text":"<p>Each level is a standalone challenge with its own Codespace that builds on the story while being technically independent \u2014 pick your level and start wherever you feel comfortable!</p> <p>\ud83d\udca1 Not sure which level to choose? Learn more about levels</p>"},{"location":"01-echoes-lost-in-orbit/#beginner-broken-echoes","title":"\ud83d\udfe2 Beginner: Broken Echoes","text":"<p>Status: \u2705 Available Topics: ArgoCD ApplicationSets, GitOps fundamentals</p> <p>The Echo Server is misbehaving. Both environments seem to be down, and messages are silent. Your mission: investigate the ArgoCD configuration and restore proper multi-environment delivery.</p> <p>Start the Beginner Challenge</p>"},{"location":"01-echoes-lost-in-orbit/#intermediate-the-silent-canary","title":"\ud83d\udfe1 Intermediate: The Silent Canary","text":"<p>Status: \u2705 Available Topics: Argo Rollouts, Progressive Delivery, Prometheus</p> <p>After fixing the communication outage, the Intergalactic Union welcomed a new species: the Zephyrians. The communications team attempted to deploy their language files using a progressive delivery system, but the rollout is failing. Your mission: debug the broken canary deployment and bring the Zephyrians' voices online.</p> <p>Start the Intermediate Challenge</p>"},{"location":"01-echoes-lost-in-orbit/#expert-echoes-in-the-dark","title":"\ud83d\udd34 Expert: Echoes in the Dark","text":"<p>Status: \u2705 Available Topics: Argo Rollouts, Progressive Delivery, Prometheus, Open Telemetry, Jaeger</p> <p>After fixing the Zephyrian communications, word of your progressive release mastery spread across the galaxy. The Bytari, a highly advanced species from the Andromeda sector, were impressed. \ud83c\udf1f</p> <p>They want to apply progressive delivery to their mission-critical service: HotROD (Hyperspace Operations &amp; Transport - Rapid Orbital Dispatch), an interstellar ride-sharing service handling dispatch requests across thousands of star systems. Every millisecond of latency matters, and any error could strand travelers between dimensions.</p> <p>Start the Expert Challenge</p>"},{"location":"01-echoes-lost-in-orbit/beginner/","title":"\ud83d\udfe2 Beginner: Broken Echoes","text":"<p>The Echo Server is misbehaving. Both environments seem to be down, and messages are silent. Your mission: investigate the ArgoCD configuration and restore proper multi-environment delivery.</p>"},{"location":"01-echoes-lost-in-orbit/beginner/#deadline","title":"\u23f0 Deadline","text":"<p>Wednesday, 10 December 2025 at 09:00 CET</p> <p>\u2139\ufe0f You can still complete the challenge after this date, but points will only be awarded for submissions before the deadline.</p>"},{"location":"01-echoes-lost-in-orbit/beginner/#solution-walkthrough","title":"\ud83d\udcdd Solution Walkthrough","text":"<p>\u26a0\ufe0f Spoiler Alert: The following walkthrough contains the full solution to the challenge. We encourage you to try solving it on your own first. Consider coming back here only if you get stuck or want to check your approach.</p> <p>Need help restoring multi-environment delivery? Follow the step-by-step beginner solution walkthrough to learn how to:</p> <ul> <li>Investigate the Argo CD ApplicationSet and spot common pitfalls</li> <li>Adjust the Argo CD ApplicationSet to meet the challenge objective</li> <li>Understand the reasoning behind each change, not just the commands</li> </ul> <p>The guide is written to explain not just what to do, but why. Dive in and level up your GitOps skills!</p>"},{"location":"01-echoes-lost-in-orbit/beginner/#join-the-discussion","title":"\ud83d\udcac Join the discussion","text":"<p>Share your solutions and questions in the challenge thread in the Open Ecosystem Community.</p>"},{"location":"01-echoes-lost-in-orbit/beginner/#objective","title":"\ud83c\udfaf Objective","text":"<p>By the end of this level, you should:</p> <ul> <li>See two distinct Applications in the Argo CD dashboard (one per environment)</li> <li>Ensure each Application deploys to its own isolated namespace</li> <li>Make the system resilient so Argo CD automatically reverts manual changes made to the cluster</li> <li>Confirm that updates happen automatically without leaving stale resources behind</li> </ul>"},{"location":"01-echoes-lost-in-orbit/beginner/#what-youll-learn","title":"\ud83e\udde0 What You'll Learn","text":"<ul> <li>How Argo CD ApplicationSets work</li> <li>How to reason about templating and sync policies</li> <li>How drift detection and self-healing operate in GitOps workflows</li> </ul>"},{"location":"01-echoes-lost-in-orbit/beginner/#toolbox","title":"\ud83e\uddf0 Toolbox","text":"<p>Your Codespace comes pre-configured with the following tools to help you solve the challenge:</p> <ul> <li><code>kubectl</code>: The Kubernetes command-line tool for interacting with   the cluster</li> <li><code>kubens</code>: Fast way to switch between Kubernetes namespaces</li> <li><code>k9s</code>: A terminal-based UI to interact with your Kubernetes clusters</li> </ul>"},{"location":"01-echoes-lost-in-orbit/beginner/#how-to-play","title":"\u2705 How to Play","text":""},{"location":"01-echoes-lost-in-orbit/beginner/#1-fork-the-repository","title":"1. Fork the Repository","text":"<ul> <li>Click the \"Fork\" button in the top-right corner of the GitHub repo or   use this link.</li> </ul>"},{"location":"01-echoes-lost-in-orbit/beginner/#2-start-a-codespace","title":"2. Start a Codespace","text":"<ul> <li>From your fork, click the green Code button \u2192 Codespaces hamburger menu \u2192 New with options.   </li> <li>Select the Adventure 01 | \ud83d\udfe2 Beginner (Broken Echoes) configuration.   </li> </ul> <p>\u26a0\ufe0f Important: The challenge will not work if you choose another configuration (or the default).</p>"},{"location":"01-echoes-lost-in-orbit/beginner/#3-wait-for-infrastructure-to-deploy","title":"3. Wait for Infrastructure to Deploy","text":"<ul> <li>Your Codespace will automatically provision a Kubernetes cluster, Argo CD, and the sample app. This usually takes   around 5 minutes.</li> </ul> <p>\ud83d\udca1 Tip: To check the progress press <code>Cmd + Shift + P</code> (or <code>Ctrl + Shift + P</code> on Windows/Linux) and search for <code>View Creation Log</code> (available after a few moments once the Codespace has initialized).</p>"},{"location":"01-echoes-lost-in-orbit/beginner/#4-access-the-argo-cd-dashboard","title":"4. Access the Argo CD Dashboard","text":"<ul> <li>Open the Ports tab in the bottom panel</li> <li>Find the Argo CD row (port <code>30100</code>) and click the forwarded address</li> </ul> <ul> <li>Log in using:   <pre><code>Username: readonly\nPassword: a-super-secure-password\n</code></pre></li> </ul>"},{"location":"01-echoes-lost-in-orbit/beginner/#5-fix-the-configuration","title":"5. Fix the Configuration","text":"<ul> <li>All errors are located in this ApplicationSet:   <pre><code>adventures/01-echoes-lost-in-orbit/beginner/manifests/appset.yaml\n</code></pre></li> <li>Learn more about ApplicationSets and   the Application Specification in   the ArgoCD docs.</li> </ul> <p>\ud83d\udce6 About Kustomize: This challenge uses Kustomize under the hood to manage Kubernetes manifests. Kustomize allows us to maintain a base set of manifests (deployment, service) and apply environment-specific customizations through overlays (staging, prod). Each overlay can modify the base configuration\u2014like changing replica counts or namespaces\u2014without duplicating YAML. Argo CD automatically detects and applies these Kustomize configurations, so you don't need to run Kustomize commands manually. Your focus is on fixing the ApplicationSet to properly reference these Kustomize-managed paths.</p> <ul> <li>After making changes, apply them:   <pre><code>kubectl apply -n argocd -f adventures/01-echoes-lost-in-orbit/beginner/manifests/appset.yaml\n</code></pre>   (Run from the repo root)</li> </ul>"},{"location":"01-echoes-lost-in-orbit/beginner/#6-verify-your-solution","title":"6. Verify Your Solution","text":"<p>Once you think you've solved the challenge, it's time to verify!</p>"},{"location":"01-echoes-lost-in-orbit/beginner/#run-the-smoke-test","title":"Run the Smoke Test","text":"<p>Run the provided smoke test script from the repo root:</p> <pre><code>adventures/01-echoes-lost-in-orbit/beginner/smoke-test.sh\n</code></pre> <p>If the test passes, your solution is very likely correct! \ud83c\udf89</p>"},{"location":"01-echoes-lost-in-orbit/beginner/#complete-full-verification","title":"Complete Full Verification","text":"<p>For comprehensive validation and to officially claim completion:</p> <ol> <li>Commit and push your changes to your fork</li> <li>Manually trigger the verification workflow on GitHub Actions</li> <li>Share your success with    the community</li> </ol> <p>\ud83d\udcd6 Need detailed verification instructions? Check out the Verification Guide for step-by-step instructions on both smoke tests and GitHub Actions workflows.</p>"},{"location":"01-echoes-lost-in-orbit/expert/","title":"\ud83d\udd34 Expert: Hyperspace Operations &amp; Transport","text":"<p>After fixing the Zephyrian communications, word of your progressive release mastery spread across the galaxy. The Bytari, a highly advanced species from the Andromeda sector, were impressed. \ud83c\udf1f</p> <p>They want to apply progressive delivery to their mission-critical service: HotROD (Hyperspace Operations &amp; Transport - Rapid Orbital Dispatch), an interstellar ride-sharing service handling dispatch requests across thousands of star systems. Every millisecond of latency matters, and any error could strand travelers between dimensions.</p> <p>Here's the catch: a previous engineer started instrumenting HotROD with OpenTelemetry and configured Argo Rollouts for automated validation, but left the setup incomplete. The observability pipeline is broken.</p> <p>The Bytari don't use staging/production environments\u2014they believe in single-environment progressive delivery validated purely by trace-derived metrics and automated health checks.</p> <p>Your mission: Fix the observability pipeline and canary validation. Make HotROD deployment-ready with proper distributed tracing.</p>"},{"location":"01-echoes-lost-in-orbit/expert/#deadline","title":"\u23f0 Deadline","text":"<p>Wednesday, 14 January 2026 at 09:00 CET</p> <p>\u2139\ufe0f You can still complete the challenge after this date, but points will only be awarded for submissions before the deadline.</p>"},{"location":"01-echoes-lost-in-orbit/expert/#join-the-discussion","title":"\ud83d\udcac Join the discussion","text":"<p>Share your solutions and questions in the challenge thread in the Open Ecosystem Community.</p>"},{"location":"01-echoes-lost-in-orbit/expert/#objective","title":"\ud83c\udfaf Objective","text":"<p>By the end of this level, you should have:</p> <ul> <li>Automated rollout progression to HotROD version <code>1.76.0</code> driven by observability signals</li> <li>OpenTelemetry Collector configured with:<ul> <li>OTLP receiver for traces from HotROD</li> <li>Spanmetrics connector converting traces as metrics</li> <li>Trace export to Jaeger, metrics export to Prometheus</li> </ul> </li> <li>Canary analysis validating deployments with 3 queries:<ul> <li>Traffic detection ensuring minimum request rate (&gt;= 0.05 req/s) to the canary to prevent \"idle canaries\" that get promoted but never had real traffic. You can use the <code>hotrod_requests_total</code> metric to verify this</li> <li>Error rate thresholds (&lt; 5%)</li> <li>Latency thresholds for the 95th percentile (&lt; 1000ms)</li> </ul> </li> </ul> <p>The Bytari engineer who started this setup left an architecture diagram that should help you getting started:</p> <p></p>"},{"location":"01-echoes-lost-in-orbit/expert/#what-youll-learn","title":"\ud83e\udde0 What You'll Learn","text":"<ul> <li>Configure OpenTelemetry Collector pipelines (receivers, connectors, exporters)</li> <li>Use the Span Metrics Connector to convert traces into metrics</li> <li>Prevent \"idle canaries\" that deploy successfully but were never really tested because they received no traffic</li> <li>Integrate distributed tracing for automated rollout decisions</li> <li>Write PromQL queries based on app and trace-derived metrics</li> </ul>"},{"location":"01-echoes-lost-in-orbit/expert/#toolbox","title":"\ud83e\uddf0 Toolbox","text":"<p>Your Codespace comes pre-configured with the following tools to help you solve the challenge:</p> <ul> <li><code>kubectl</code>: The Kubernetes command-line tool for interacting with the   cluster</li> <li><code>kubens</code>: Fast way to switch between Kubernetes namespaces</li> <li><code>k9s</code>: A terminal-based UI to interact with your Kubernetes clusters</li> <li>Argo CD CLI: Manage Argo CD applications from   the command line</li> <li>Argo Rollouts kubectl plugin: Extended   kubectl commands for managing Argo rollouts</li> </ul>"},{"location":"01-echoes-lost-in-orbit/expert/#how-to-play","title":"\u2705 How to Play","text":""},{"location":"01-echoes-lost-in-orbit/expert/#1-start-your-challenge","title":"1. Start Your Challenge","text":"<p>\ud83d\udcd6 First time? Check out the Getting Started Guide for detailed instructions on forking, starting a Codespace, and waiting for infrastructure setup.</p> <p>Quick start:</p> <ul> <li>Fork the repo</li> <li>Create a Codespace</li> <li>Select \"Adventure 01 | \ud83d\udd34 Expert (Hyperspace Operations &amp; Transport)\"</li> <li>Wait ~5-10 minutes for all infrastructure to deploy (<code>Cmd/Ctrl + Shift + P</code> \u2192 <code>View Creation Log</code> to view progress)</li> </ul> <p>\u26a0\ufe0f After the infrastructure deploys, the setup script automatically starts port forwarding to the Argo Rollouts dashboard. This keeps the terminal busy, which is expected behavior. Your environment is fully ready when you see the terminal output shown below. Just open a new terminal to run commands.</p> <p></p>"},{"location":"01-echoes-lost-in-orbit/expert/#2-access-the-uis","title":"2. Access the UIs","text":"<ul> <li>Open the Ports tab in the bottom panel to access the following UIs</li> </ul> <p>\ud83d\udca1 Not a fan of user interfaces? No problem, you can also use the CLI tools to complete the challenge. But if you're new(ish) to these tools, the UIs can help you get familiar faster.</p>"},{"location":"01-echoes-lost-in-orbit/expert/#argo-cd-port-30100","title":"Argo CD (Port 30100)","text":"<p>The Argo CD UI shows the sync status of your applications and allows you to refresh them after pushing new commits.</p> <ul> <li>Find the Argo CD row (port 30100) and click the forwarded address</li> <li>Log in using:   <pre><code>Username: readonly\nPassword: a-super-secure-password\n</code></pre></li> </ul>"},{"location":"01-echoes-lost-in-orbit/expert/#argo-rollouts-port-30101","title":"Argo Rollouts (Port 30101)","text":"<p>The Argo Rollouts dashboard shows canary deployment progress and analysis status.</p> <ul> <li>Find the Argo Rollouts row (port 30101) and click the forwarded address</li> </ul>"},{"location":"01-echoes-lost-in-orbit/expert/#prometheus-port-30102","title":"Prometheus (Port 30102)","text":"<p>The Prometheus UI helps you explore available metrics and test your PromQL queries.</p> <ul> <li>Find the Prometheus row (port 30102) and click the forwarded address</li> </ul>"},{"location":"01-echoes-lost-in-orbit/expert/#jaeger-port-30103","title":"Jaeger (Port 30103)","text":"<p>The Jaeger UI shows distributed traces from HotROD. You can use it to verify that tracing is working end-to-end.</p> <ul> <li>Find the Jaeger row (port 30103) and click the forwarded address</li> </ul>"},{"location":"01-echoes-lost-in-orbit/expert/#3-fix-the-configuration","title":"3. Fix the Configuration","text":"<p>The Bytari are counting on you. The HotROD service is deployed but the observability pipeline is broken, preventing new releases. Your task is to investigate, identify, and fix the issues.</p> <p>Review the \ud83c\udfaf Objective section to understand what a successful solution looks like. The architecture diagram above shows how the components should connect. Use the Argo Rollouts dashboard, Prometheus UI, and Jaeger UI to debug and validate your changes.</p>"},{"location":"01-echoes-lost-in-orbit/expert/#where-to-look","title":"Where to Look","text":"<p>All manifests are located in:</p> <pre><code>adventures/01-echoes-lost-in-orbit/expert/manifests/\n</code></pre>"},{"location":"01-echoes-lost-in-orbit/expert/#deploy-your-changes","title":"Deploy Your Changes","text":"<p>After making your fixes, commit and push them to trigger the deployment:</p> <pre><code>git add adventures/01-echoes-lost-in-orbit/expert/manifests/\ngit commit -m \"Fix configuration\"\ngit push\n</code></pre> <p>\ud83d\udca1 Tip: If you're pushing to a branch other than <code>main</code>, make sure to also update the <code>ApplicationSet</code> in <code>adventures/01-echoes-lost-in-orbit/expert/manifests/appset.yaml</code> to point to your branch.</p> <p>Argo CD will automatically sync your changes after some time. You can speed things up by refreshing the applications manually. Depending on what you changed, use one of the following commands:</p> <pre><code>argocd app get hotrod --refresh\nargocd app get otel --refresh\n</code></pre> <p>If you made changes to HotROD, trigger a new rollout after ArgoCD synced your changes:</p> <pre><code>kubectl argo rollouts retry rollout hotrod -n hotrod\n</code></pre> <p>If you made changes to the Open Telemetry Collector Config, make sure to restart the collector for them to take effect:</p> <pre><code>kubectl rollout restart daemonset/collector -n otel\n</code></pre>"},{"location":"01-echoes-lost-in-orbit/expert/#monitor-the-rollout","title":"Monitor the Rollout","text":"<p>Watch the canary deployment progress in the Argo Rollouts dashboard or use the CLI:</p> <pre><code>kubectl argo rollouts get rollout hotrod -n hotrod --watch\n</code></pre> <p>The rollout should automatically progress through the canary stages based on the analysis metrics validation.</p>"},{"location":"01-echoes-lost-in-orbit/expert/#helpful-documentation","title":"Helpful Documentation","text":"<ul> <li>OpenTelemetry Collector Configuration</li> <li>Span Metrics Connector</li> <li>Argo Rollouts Analysis</li> <li>PromQL Basics</li> </ul>"},{"location":"01-echoes-lost-in-orbit/expert/#4-verify-your-solution","title":"4. Verify Your Solution","text":"<p>Once you think you've solved the challenge, it's time to verify!</p>"},{"location":"01-echoes-lost-in-orbit/expert/#run-the-smoke-test","title":"Run the Smoke Test","text":"<p>Run the provided smoke test script from the repo root:</p> <pre><code>adventures/01-echoes-lost-in-orbit/expert/smoke-test.sh\n</code></pre> <p>If the test passes, your solution is very likely correct! \ud83c\udf89</p>"},{"location":"01-echoes-lost-in-orbit/expert/#complete-full-verification","title":"Complete Full Verification","text":"<p>For comprehensive validation and to officially claim completion:</p> <ol> <li>Commit and push your changes to your fork</li> <li>Manually trigger the verification workflow on GitHub Actions</li> <li>Share your success with the community</li> </ol> <p>\ud83d\udcd6 Need detailed verification instructions? Check out the Verification Guide for step-by-step instructions on both smoke tests and GitHub Actions workflows.</p>"},{"location":"01-echoes-lost-in-orbit/intermediate/","title":"\ud83d\udfe1 Intermediate: The Silent Canary","text":"<p>After fixing the communication outage in Level 1, the Intergalactic Union welcomed a new species: the Zephyrians \ud83c\udf1f</p> <p>The communications team attempted to deploy their language files using a progressive delivery system, but the rollout is failing. The Zephyrians are still waiting to communicate with the rest of the galaxy.</p> <p>A previous engineer configured automated canary deployments with health checks but left the setup incomplete. Your mission: debug the broken rollout and bring the Zephyrians' voices online.</p>"},{"location":"01-echoes-lost-in-orbit/intermediate/#deadline","title":"\u23f0 Deadline","text":"<p>Wednesday, 24 December 2025 at 09:00 CET</p> <p>\u2139\ufe0f You can still complete the challenge after this date, but points will only be awarded for submissions before the deadline.</p>"},{"location":"01-echoes-lost-in-orbit/intermediate/#join-the-discussion","title":"\ud83d\udcac Join the discussion","text":"<p>Share your solutions and questions in the challenge thread in the Open Ecosystem Community.</p>"},{"location":"01-echoes-lost-in-orbit/intermediate/#objective","title":"\ud83c\udfaf Objective","text":"<p>By the end of this level, you should have:</p> <ul> <li>Pod info version 6.9.3 deployed successfully in both staging and production environments</li> <li>Rollouts automatically progress through canary stages based on health metrics</li> <li>Two working PromQL queries in the <code>AnalysisTemplate</code> that validate application health during releases</li> <li>All rollouts complete successfully</li> </ul>"},{"location":"01-echoes-lost-in-orbit/intermediate/#what-youll-learn","title":"\ud83e\udde0 What You'll Learn","text":"<ul> <li>How to write PromQL queries to monitor application health</li> <li>How progressive delivery reduces deployment risk with automated validation</li> <li>How to debug and fix broken canary deployments</li> <li>How Argo Rollouts and Prometheus work together to   make data-driven deployment decisions</li> </ul>"},{"location":"01-echoes-lost-in-orbit/intermediate/#toolbox","title":"\ud83e\uddf0 Toolbox","text":"<p>Your Codespace comes pre-configured with the following tools to help you solve the challenge:</p> <ul> <li><code>kubectl</code>: The Kubernetes command-line tool for interacting with the   cluster</li> <li><code>kubens</code>: Fast way to switch between Kubernetes namespaces</li> <li><code>k9s</code>: A terminal-based UI to interact with your Kubernetes clusters</li> <li>Argo CD CLI: Manage Argo CD applications from   the command line</li> <li>Argo Rollouts kubectl plugin: Extended   kubectl commands for managing Argo rollouts</li> </ul>"},{"location":"01-echoes-lost-in-orbit/intermediate/#how-to-play","title":"\u2705 How to Play","text":""},{"location":"01-echoes-lost-in-orbit/intermediate/#1-start-your-challenge","title":"1. Start Your Challenge","text":"<p>\ud83d\udcd6 First time? Check out the Getting Started Guide for detailed instructions on forking, starting a Codespace, and waiting for infrastructure setup.</p> <p>Quick start:</p> <ul> <li>Fork the repo</li> <li>Create a Codespace</li> <li>Select \"Adventure 01 | \ud83d\udfe1 Intermediate (The Silent Canary)\"</li> <li>Wait ~5-10 minutes for all infrastructure to deploy (<code>Cmd/Ctrl + Shift + P</code> \u2192 <code>View Creation Log</code> to view progress)</li> </ul> <p>\u26a0\ufe0f After the infrastructure deploys, the setup script automatically starts port forwarding to the Argo Rollouts dashboard. This keeps the terminal busy, which is expected behavior. Your environment is fully ready when you see the terminal output shown below. Just open a new terminal to run commands.</p> <p></p>"},{"location":"01-echoes-lost-in-orbit/intermediate/#2-access-the-uis","title":"2. Access the UIs","text":"<ul> <li>Open the Ports tab in the bottom panel to access the following UIs   </li> </ul> <p>\ud83d\udca1 Not a fan of user interfaces? No problem, you can also use the CLI tools to complete the challenge. But if you're new(ish) to these tools, the UIs can help you get familiar faster.</p>"},{"location":"01-echoes-lost-in-orbit/intermediate/#argo-cd-port-30100","title":"Argo CD (Port 30100)","text":"<p>The Argo CD UI shows the sync status of your applications and allows you to refresh them after pushing new commits.</p> <ul> <li>Find the Argo CD row (port 30100) and click the forwarded address</li> <li>Log in using:   <pre><code>Username: readonly\nPassword: a-super-secure-password\n</code></pre></li> </ul>"},{"location":"01-echoes-lost-in-orbit/intermediate/#argo-rollouts-port-30101","title":"Argo Rollouts (Port 30101)","text":"<p>The Argo Rollouts dashboard shows canary deployment progress and analysis status.</p> <ul> <li>Find the Argo Rollouts row (port 30101) and click the forwarded address</li> </ul>"},{"location":"01-echoes-lost-in-orbit/intermediate/#prometheus-port-30102","title":"Prometheus (Port 30102)","text":"<p>The Prometheus UI helps you explore available metrics and test your PromQL queries.</p> <ul> <li>Find the Prometheus row (port 30102) and click the forwarded address</li> </ul>"},{"location":"01-echoes-lost-in-orbit/intermediate/#3-fix-the-configuration","title":"3. Fix the Configuration","text":"<p>The Zephyrians are waiting for their language pack, but there are misconfigurations preventing the rollout from completing successfully. Your task is to investigate, identify, and fix the issues.</p> <p>Review the \ud83c\udfaf Objective section to understand what a successful solution looks like. The Argo Rollouts dashboard and Prometheus UI can help you debug and validate your changes.</p>"},{"location":"01-echoes-lost-in-orbit/intermediate/#where-to-look","title":"Where to Look","text":"<p>All manifests are located in:</p> <pre><code>adventures/01-echoes-lost-in-orbit/intermediate/manifests/\n</code></pre> <p>\ud83d\udce6 About Kustomize: This challenge uses Kustomize under the hood to manage Kubernetes manifests. Kustomize allows us to maintain a base set of manifests and apply environment-specific customizations through overlays (staging, prod). Each overlay can modify the base configuration\u2014like changing replica counts or namespaces\u2014without duplicating YAML. With the <code>ApplicationSet</code>, Argo CD automatically detects and applies these Kustomize configurations, so you don't need to run Kustomize commands manually.</p>"},{"location":"01-echoes-lost-in-orbit/intermediate/#deploy-your-changes","title":"Deploy Your Changes","text":"<p>After making your fixes, commit and push them to trigger the deployment:</p> <pre><code>git add adventures/01-echoes-lost-in-orbit/intermediate/manifests/\ngit commit -m \"Fix configuration\"\ngit push\n</code></pre> <p>\ud83d\udca1 Tip: If you're pushing to a branch other than <code>main</code>, make sure to also update the <code>ApplicationSet</code> in <code>adventures/01-echoes-lost-in-orbit/intermediate/manifests/appset.yaml</code> to point to your branch.</p> <p>Argo CD will automatically sync your changes after some time. You can speed things up by refreshing the applications manually:</p> <pre><code>argocd app get echo-server-staging --refresh\nargocd app get echo-server-prod --refresh\n</code></pre> <p>After ArgoCD syncs your changes, trigger the rollout:</p> <pre><code>kubectl argo rollouts retry rollout echo-server -n echo-staging\nkubectl argo rollouts retry rollout echo-server -n echo-prod\n</code></pre>"},{"location":"01-echoes-lost-in-orbit/intermediate/#monitor-the-rollout","title":"Monitor the Rollout","text":"<p>Watch the canary deployment progress in the Argo Rollouts dashboard or use the CLI:</p> <pre><code>kubectl argo rollouts get rollout echo-server -n echo-staging --watch\nkubectl argo rollouts get rollout echo-server -n echo-prod --watch\n</code></pre> <p>The rollout should automatically progress through the canary stages (33% \u2192 66% \u2192 100%).</p> <p>\u2139\ufe0f Note: In real-world progressive delivery, updates are typically deployed to staging first, validated, and then promoted to production. For this challenge, both environments update simultaneously to simplify the workflow and focus on learning canary rollouts and health checks.</p>"},{"location":"01-echoes-lost-in-orbit/intermediate/#helpful-documentation","title":"Helpful Documentation","text":"<ul> <li>Argo Rollouts</li> <li>Analysis and Progressive Delivery</li> <li>PromQL Basics</li> <li>Kubernetes Metrics exported by   kube-state-metrics</li> </ul>"},{"location":"01-echoes-lost-in-orbit/intermediate/#4-verify-your-solution","title":"4. Verify Your Solution","text":"<p>Once you think you've solved the challenge, it's time to verify!</p>"},{"location":"01-echoes-lost-in-orbit/intermediate/#run-the-smoke-test","title":"Run the Smoke Test","text":"<p>Run the provided smoke test script from the repo root:</p> <pre><code>adventures/01-echoes-lost-in-orbit/intermediate/smoke-test.sh\n</code></pre> <p>If the test passes, your solution is very likely correct! \ud83c\udf89</p>"},{"location":"01-echoes-lost-in-orbit/intermediate/#complete-full-verification","title":"Complete Full Verification","text":"<p>For comprehensive validation and to officially claim completion:</p> <ol> <li>Commit and push your changes to your fork</li> <li>Manually trigger the verification workflow on GitHub Actions</li> <li>Share your success with the community</li> </ol> <p>\ud83d\udcd6 Need detailed verification instructions? Check out the Verification Guide for step-by-step instructions on both smoke tests and GitHub Actions workflows.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/","title":"\ud83d\udfe2 Beginner Solution: Broken Echoes","text":"<p>Congratulations on taking on the first Open Ecosystem Challenge! In this walkthrough, we'll approach the challenge exactly as you would: start with the objectives, break them down one by one, and systematically fix what's broken. \ud83d\ude80</p> <p>\u26a0\ufe0f Spoiler Alert: This walkthrough contains the full solution to the challenge. We encourage you to try solving it on your own first. Consider coming back here only if you get stuck or want to check your approach.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#step-1-review-the-challenge-objectives","title":"\ud83d\udccb Step 1: Review the Challenge Objectives","text":"<p>Let's start by reading what we need to achieve. The challenge states:</p> <p>By the end of this level, you should:</p> <ul> <li>See two distinct Applications in the Argo CD dashboard (one per environment)</li> <li>Ensure each Application deploys to its own isolated namespace</li> <li>Make the system resilient so Argo CD automatically reverts manual changes made to the cluster</li> <li>Confirm that updates happen automatically without leaving stale resources behind</li> </ul> <p>Perfect! Now we have four clear objectives to work toward. Let's tackle them one by one.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#step-2-understand-the-setup","title":"\ud83d\udd0d Step 2: Understand the Setup","text":"<p>Before we start fixing things, let's understand what we're working with. The challenge uses:</p> <ul> <li>Argo CD ApplicationSet: Automatically generates Applications for multiple environments</li> <li>Git directory generator: Scans for directories in the <code>overlays/</code> folder (finds <code>staging</code> and <code>prod</code>)</li> <li>Kustomize: Manages environment-specific configurations<ul> <li>Base: Common configuration (deployment, service)</li> <li>Overlays: Environment-specific customizations (staging and prod)</li> </ul> </li> </ul> <p>The file we need to fix is:</p> <pre><code>adventures/01-echoes-lost-in-orbit/beginner/manifests/appset.yaml\n</code></pre> <p>Let's open it:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: echo-server\n  namespace: argocd\nspec:\n  generators:\n    - git:\n        repoURL: __REPO_URL__\n        revision: HEAD\n        directories:\n          - path: adventures/01-echoes-lost-in-orbit/beginner/manifests/overlays/*\n  template:\n    metadata:\n      name: echo-server\n      labels:\n        app.kubernetes.io/managed-by: argocd\n    spec:\n      project: default\n      source:\n        repoURL: __REPO_URL__\n        targetRevision: HEAD\n        path: adventures/01-echoes-lost-in-orbit/beginner/manifests/overlays/{{path.basename}}\n      destination:\n        server: https://kubernetes.default.svc\n        namespace: echo\n      syncPolicy:\n        syncOptions:\n          - CreateNamespace=true\n</code></pre> <p>Now let's work through each objective.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#step-3-clear-objectives","title":"\ud83c\udfaf Step 3: Clear Objectives","text":""},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#objective-1-see-two-distinct-applications-in-the-argo-cd-dashboard","title":"Objective 1: See Two Distinct Applications in the Argo CD Dashboard","text":"<p>When opening the Argo CD dashboard (port <code>30100</code>), we won\u2019t see any applications.</p> <p></p> <p>Let's check if the ApplicationSet is present:</p> <pre><code>kubectl get applicationset -n argocd\n</code></pre> <p>You should see <code>echo-server</code> listed. That means Argo CD is at least aware of our ApplicationSet.</p> <p>Let's dig deeper and check the status of the ApplicationSet:</p> <pre><code>kubectl get applicationset echo-server -n argocd -o yaml\n</code></pre> <p>Scroll down to the <code>status.conditions</code> section. You should see an error like:</p> <pre><code>- message: 'ApplicationSet echo-server contains applications with duplicate name: echo-server'\n  reason: ErrorOccurred\n  status: \"False\"\n</code></pre> <p>This error means there are duplicate application names. Let's look at how the name is set in the template.</p> <p>The manifest shows:</p> <pre><code>metadata:\n  name: echo-server\n  namespace: argocd\n</code></pre> <p>For each environment (staging and prod), Argo CD tries to create an application called <code>echo-server</code>. This fails because Kubernetes resources must have unique names in a namespace, but the ApplicationSet template uses a static name for every generated Application. The Git directory generator creates one Application per overlay directory, so you need a unique name for each.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#how-to-fix","title":"How to fix?","text":"<p>Let's update the name field to use a template variable:</p> <pre><code>name: echo-server-{{path.basename}}\n</code></pre> <p>This way, Argo CD will generate <code>echo-server-staging</code> and <code>echo-server-prod</code>. One for each environment. The <code>{{path.basename}}</code> variable is replaced with the directory name (e.g., <code>staging</code> or <code>prod</code>) for each overlay, ensuring uniqueness.</p> <p>Now, let's apply the fix:</p> <pre><code>kubectl apply -n argocd -f adventures/01-echoes-lost-in-orbit/beginner/manifests/appset.yaml\n</code></pre> <p>Go back to the dashboard. You should now see two progressing applications.</p> <p></p>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>ApplicationSets use templates to generate multiple Applications, and names must be unique per environment.</li> <li>The Git directory generator creates one Application per overlay directory, so template variables like <code>{{path.basename}}</code> are essential for dynamic naming.</li> <li>The ApplicationSet and Application status are helpful for troubleshooting errors.</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#further-reading","title":"Further Reading","text":"<ul> <li>Argo CD ApplicationSet Documentation</li> <li>ApplicationSet Templates</li> <li>Git Directory Generator</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#objective-2-ensure-each-application-deploys-to-its-own-isolated-namespace","title":"Objective 2: Ensure each Application deploys to its own isolated namespace","text":"<p>We now have two applications showing up in Argo CD. But they are both deploying to the same namespace: <code>echo</code>.</p> <p>Each environment should at least have its own namespace for isolation and to avoid resource conflicts. This can be achieved by using a template variable in the namespace field, just like with the application name.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#how-to-fix_1","title":"How to fix?","text":"<p>Update the namespace field in the ApplicationSet manifest to include the <code>{{path.basename}}</code> variable:</p> <pre><code>spec.template.spec.destination.namespace: echo-{{path.basename}}\n</code></pre> <p>Now, let's apply the fix:</p> <pre><code>kubectl apply -n argocd -f adventures/01-echoes-lost-in-orbit/beginner/manifests/appset.yaml\n</code></pre> <p>This will set the destination namespace to <code>echo-staging</code> and <code>echo-prod</code> for the respective environments.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#key-takeaways_1","title":"Key Takeaways","text":"<ul> <li>How to use template variables for namespace isolation in multi-environment setups.</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#further-reading_1","title":"Further Reading","text":"<ul> <li>Kubernetes Namespaces</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#objective-3-make-the-system-resilient-so-argo-cd-automatically-reverts-manual-changes-made-to-the-cluster","title":"Objective 3: Make the system resilient so Argo CD automatically reverts manual changes made to the cluster","text":"<p>Our Applications are now deploying to separate namespaces, but they are not syncing yet and users can still make manual changes to the cluster. Let's split this objective into two clear steps:</p>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#fix-1-enable-auto-sync","title":"Fix 1: Enable auto sync","text":"<p>By default, Argo CD does not automatically sync Applications. This means changes in Git are not applied to the cluster unless you manually trigger a sync (which is disabled for this challenge). To fix this, let's enable auto sync in the ApplicationSet manifest:</p> <pre><code>syncPolicy:\n  automated:\n    enabled: true\n  syncOptions:\n    - CreateNamespace=true\n</code></pre> <p>Apply the change:</p> <pre><code>kubectl apply -n argocd -f adventures/01-echoes-lost-in-orbit/beginner/manifests/appset.yaml\n</code></pre> <p>Now, Argo CD will automatically sync the Applications whenever there are changes in Git.</p> <p></p> <p>\ud83d\udca1 By default, this takes up to 3 minutes to detect changes. can speed this up by configuring a webhook to ArgoCD but for this challenge manually refreshing is just fine.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#fix-2-enable-self-healing-automatic-reverts","title":"Fix 2: Enable self-healing (automatic reverts)","text":"<p>Auto sync alone will not revert manual changes made directly in the cluster. To make Argo CD truly resilient, let's enable self-healing. This will ensure that any drift from the desired state in Git is automatically corrected.</p> <p>Update the ApplicationSet manifest to add <code>selfHeal: true</code>:</p> <pre><code>syncPolicy:\n  automated:\n    enabled: true\n    selfHeal: true\n  syncOptions:\n    - CreateNamespace=true\n</code></pre> <p>Apply the change:</p> <pre><code>kubectl apply -n argocd -f adventures/01-echoes-lost-in-orbit/beginner/manifests/appset.yaml\n</code></pre> <p>Let's test self-healing by making a manual change:</p> <pre><code>kubectl scale deployment echo-server-staging -n echo-staging --replicas=3\nkubectl get pods -n echo-staging -w\n</code></pre> <p>Within seconds, Argo CD will detect the drift and scale it back down to 1 replica (as defined in Git).</p>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#key-takeaways_2","title":"Key Takeaways","text":"<ul> <li>Auto sync ensures that changes in Git are automatically applied to the cluster.</li> <li>Self-healing ensures the cluster matches the desired state in Git, even after manual changes.</li> <li>Auto sync and self-heal together enforce GitOps and keep your cluster consistent.</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#further-reading_2","title":"Further Reading","text":"<ul> <li>Argo CD Automated Sync Policy</li> <li>Self-Healing in Argo CD</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#objective-4-confirm-that-updates-happen-automatically-without-leaving-stale-resources-behind","title":"Objective 4: Confirm that updates happen automatically without leaving stale resources behind","text":"<p>Now that our applications can sync automatically and self-heal, let's make sure that deleting resources from Git also removes them from the cluster. Otherwise, we risk leaving behind stale resources that are no longer needed.</p> <p>By default, Argo CD does not remove resources from the cluster when they are deleted from Git. Pruning must be explicitly enabled to keep the cluster clean and in sync with Git.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#how-to-fix_2","title":"How to fix?","text":"<p>Let's enable pruning by adding <code>prune: true</code> to the automated sync policy in the ApplicationSet manifest:</p> <pre><code>syncPolicy:\n  automated:\n    enabled: true\n    selfHeal: true\n    prune: true\n  syncOptions:\n    - CreateNamespace=true\n</code></pre> <p>Apply the change:</p> <pre><code>kubectl apply -n argocd -f adventures/01-echoes-lost-in-orbit/beginner/manifests/appset.yaml\n</code></pre> <p>Now, Argo CD will automatically delete resources from the cluster when they're removed from Git.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#key-takeaways_3","title":"Key Takeaways","text":"<ul> <li>Pruning (in combination with self-heal) ensures the cluster matches Git exactly.</li> <li>Automated sync policies with pruning keep environments clean and up to date.</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#further-reading_3","title":"Further Reading","text":"<ul> <li>Argo CD Pruning</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#complete-solution","title":"\u2705 Complete Solution","text":"<p>Here's what your corrected ApplicationSet should look like with all fixes applied:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: echo-server\n  namespace: argocd\nspec:\n  generators:\n    - git:\n        repoURL: __REPO_URL__\n        revision: HEAD\n        directories:\n          - path: adventures/01-echoes-lost-in-orbit/beginner/manifests/overlays/*\n  template:\n    metadata:\n      name: echo-server-{{path.basename}}  # \u2705 Unique name per environment\n      labels:\n        app.kubernetes.io/managed-by: argocd\n    spec:\n      project: default\n      source:\n        repoURL: __REPO_URL__\n        targetRevision: HEAD\n        path: adventures/01-echoes-lost-in-orbit/beginner/manifests/overlays/{{path.basename}}\n      destination:\n        server: https://kubernetes.default.svc\n        namespace: echo-{{path.basename}}  # \u2705 Environment-specific namespace\n      syncPolicy:\n        automated: \n          enabled: true     # \u2705 Automatic sync enabled\n          selfHeal: true    # \u2705 Reverts manual changes\n          prune: true       # \u2705 Deletes removed resources\n        syncOptions:\n          - CreateNamespace=true\n</code></pre> <p>That's it! Your ApplicationSet is now ready for resilient, automated, and clean multi-environment deployments with Argo CD.</p> <p>If you want to go further, use this setup to experiment with ArgoCD, or explore the intermediate challenge next.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/","title":"\ud83d\udfe1 Intermediate Solution Walkthrough: The Silent Canary","text":"<p>In this walkthrough, we'll approach the challenge exactly as you would: start with the objectives, break them down one by one, and systematically fix what's broken. \ud83d\ude80</p> <p>\u26a0\ufe0f Spoiler Alert: This walkthrough contains the full solution to the challenge. We encourage you to try solving it on your own first. Consider coming back here only if you get stuck or want to check your approach.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#step-1-review-the-challenge-objectives","title":"\ud83d\udccb Step 1: Review the Challenge Objectives","text":"<p>Let's start by reading what we need to achieve. The challenge states:</p> <p>By the end of this level, you should have:</p> <ul> <li>Pod info version 6.9.3 deployed successfully in both staging and production environments</li> <li>Rollouts automatically progress through canary stages based on health metrics</li> <li>Two working PromQL queries in the AnalysisTemplate that validate application health during releases</li> <li>All rollouts complete successfully</li> </ul> <p>Perfect! Now we have four clear objectives to work toward. Let's tackle them one by one.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#step-2-understand-the-setup","title":"\ud83d\udd0d Step 2: Understand the Setup","text":"<p>Before we start fixing things, let's understand what we're working with. All files are located in the following path:</p> <pre><code>adventures/01-echoes-lost-in-orbit/intermediate/manifests\n</code></pre> <p>Let's see what it contains:</p> <ul> <li><code>appset.yaml</code>: An Argo CD ApplicationSet that generates Applications for staging and prod using the Git directory generator</li> <li><code>base/</code>: The base configuration for the echo-server app which is deployed using Kustomize<ul> <li><code>analysis-template.yaml</code>: An Argo Rollouts AnalysisTemplate that defines health checks during canary deployments</li> <li><code>rollout.yaml</code>: An Argo Rollouts Rollout resource that manages the canary deployment strategy</li> <li><code>service.yaml</code>: A Kubernetes Service that exposes the echo-server app</li> <li><code>kustomization.yaml</code>: Kustomize configuration file for the base</li> </ul> </li> <li><code>overlays/</code>: Environment-specific overlays for staging and prod<ul> <li><code>kustomization.yaml</code>: Kustomize configuration that adjust the number of replicas for each overlay</li> </ul> </li> </ul> <p>Now let's work through each objective.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#step-3-clear-objectives","title":"\ud83c\udfaf Step 3: Clear Objectives","text":"<p>Note: All steps in this guide use the staging environment. Since staging and production are identical (except for the number of replicas), you can follow the same steps for both. To keep things simple, we'll only mention staging throughout.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#objective-1-pod-info-version-693-deployed-successfully-in-both-staging-and-production-environments","title":"Objective 1: Pod info version 6.9.3 deployed successfully in both staging and production environments","text":"<p>Let's start by checking which version of the podinfo image is currently running in staging:</p> <pre><code>kubectl -n echo-staging get rollout echo-server -o yaml\n</code></pre> <p>Let's look for the <code>spec.template.spec.containers[0].image</code> field. It should be set to <code>stefanprodan/podinfo:6.9.3</code>. Perfect, that's the version we want. However, if we scroll down to the <code>status</code> section, we'll notice an error:</p> <pre><code>  - message: 'Rollout aborted update to revision 2: Metric \"container-restarts\" assessed\n      Error due to consecutiveErrors (1) &gt; consecutiveErrorLimit (0): \"Error Message:\n      Post \"http://prom-server.prometheus.svc.cluster.local/api/v1/query\": dial tcp:\n      lookup prom-server.prometheus.svc.cluster.local on 10.96.0.10:53: no such host\"'\n    reason: RolloutAborted\n    status: \"False\"\n    type: Progressing\n</code></pre> <p>Let's find out which version is actually running. The rollout status includes a field called <code>status.stableRS</code>, which tells us the unique identifier of the replicaset that is currently considered stable (i.e., the one serving traffic). To understand what is actually running in our cluster, we can inspect this replicaset directly:</p> <pre><code># status.stableRS: 6fdd67656d\nkubectl -n echo-staging get replicaset echo-server-6fdd67656d -o yaml\n</code></pre> <p>Let's look for the spec.template.spec.containers[0].image field in the output. In this case, we see:</p> <pre><code>spec:\n  template:\n    spec:\n      containers:\n        - name: echo-server\n          image: stefanprodan/podinfo:6.8.0\n</code></pre> <p>This tells us that the old image (<code>6.8.0</code>) is still running, even though we expect <code>6.9.3</code>. This means that the rollout is stuck and hasn't progressed to the new version.</p> <p>If we open the Argo Rollouts UI and select the <code>echo-staging</code> namespace in the top right corner, we'll see an error in the rollout\u2014this matches the output from the previous command.</p> <p></p>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#how-to-fix","title":"How to fix?","text":"<p>We don't need to push image 6.9.3 because that has already been done. The problem is that the rollout is aborted due to an error. Let's move on to the next objective to investigate further.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Argo Rollouts won't progress a rollout if there are errors in the configuration.</li> <li>Check the rollout status and conditions for errors when rollouts don't progress.</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#further-reading","title":"Further Reading","text":"<ul> <li>Progressive Delivery with Argo Rollouts</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#objective-2-rollouts-automatically-progress-through-canary-stages-based-on-health-metrics","title":"Objective 2: Rollouts automatically progress through canary stages based on health metrics","text":"<p>As described in the previous objective, our rollout is currently not progressing due to an error. Let's investigate exactly what's going on.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#how-to-fix_1","title":"How to fix?","text":"<p>First, let's take a closer look at the error message by running:</p> <pre><code>kubectl -n echo-staging get rollout echo-server -o yaml\n</code></pre> <p>In the output, we see this condition:</p> <pre><code>  - message: 'Rollout aborted update to revision 2: Metric \"container-restarts\" assessed\n      Error due to consecutiveErrors (1) &gt; consecutiveErrorLimit (0): \"Error Message:\n      Post \"http://prom-server.prometheus.svc.cluster.local/api/v1/query\": dial tcp:\n      lookup prom-server.prometheus.svc.cluster.local on 10.96.0.10:53: no such host\"'\n    reason: RolloutAborted\n    status: \"False\"\n    type: Progressing\n</code></pre> <p>This tells us that the <code>container-restarts</code> metric is failing because it can't reach the Prometheus server. This is likely due to an incorrect URL in our AnalysisTemplate.</p> <p>Let's check the AnalysisTemplate by running:</p> <pre><code>kubectl -n echo-staging get analysistemplate echo-analysis -o yaml\n</code></pre> <p>Or, we can simply look at the file in the repo at <code>adventures/01-echoes-lost-in-orbit/intermediate/manifests/base/analysis-template.yaml</code>.</p> <p>We see that the spec contains two metrics: <code>container-restarts</code> and <code>ready-containers</code>.</p> <pre><code>spec:\n  args:\n  - name: namespace\n  metrics:\n  - consecutiveErrorLimit: 0\n    count: 1\n    failureLimit: 0\n    inconclusiveLimit: 0\n    name: container-restarts\n    provider:\n      prometheus:\n        address: http://prom-server.prometheus.svc.cluster.local\n        query: |\n          # There should be no restarts\n          sum(increase(kube_pod_container_status_restarts_total{\n            namespace=\"{{args.namespace}}\",\n            pod=~\"echo-server-.*\"\n          }[1m])) or vector(0)\n    successCondition: result[0] &gt; 0\n  - consecutiveErrorLimit: 0\n    count: 1\n    failureLimit: 0\n    inconclusiveLimit: 0\n    name: ready-containers\n    provider:\n      prometheus:\n        address: http://prometheus-server.prometheus.svc.cluster.local\n        query: |-\n          # Check how many containers are ready (should be at least 1)\n          # Look at kube_pod_container_status_* metrics\n          # Test in Prometheus UI (port 30102 in the \"Ports\" tab of VS Code)\n    successCondition: result[0] &gt;= 1\n</code></pre> <p>According to the error message, the problem is with the <code>container-restarts</code> metric: it can't reach the Prometheus server at <code>http://prom-server.prometheus.svc.cluster.local</code>.</p> <p>Let's check if this service exists. The URL follows the structure <code>http://&lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local</code>, so we run:</p> <pre><code>kubectl -n prometheus get service\n</code></pre> <p>This outputs one service called <code>prometheus-server</code>, not <code>prom-server</code>. It looks like there's a typo in our manifest. Let's fix it by changing <code>prom-server</code> to <code>prometheus-server</code> in <code>adventures/01-echoes-lost-in-orbit/intermediate/manifests/base/analysis-template.yaml</code>.</p> <p>After making the change, let's commit and push. For more details, see the challenge instructions.</p> <p>Now, let's retry the rollout:</p> <pre><code>kubectl argo rollouts retry rollout echo-server -n echo-staging\n</code></pre> <p>This time, instead of using plain <code>kubectl</code>, let's use the Argo Rollouts kubectl plugin for more detailed status:</p> <pre><code>kubectl argo rollouts -n echo-staging status echo-server\n</code></pre> <p>We see that the rollout is still not progressing:</p> <pre><code>Degraded - RolloutAborted: Rollout aborted update to revision 2: Metric \"container-restarts\" assessed Failed due to failed (1) &gt; failureLimit (0)\nError: The rollout is in a degraded state with message: RolloutAborted: Rollout aborted update to revision 2: Metric \"container-restarts\" assessed Failed due to failed (1) &gt; failureLimit (0)\n</code></pre> <p>At least now we have a new error! The <code>container-restarts</code> metric is failing because there was one failure, but the failure limit for the rollout to proceed is set to 0.</p> <p>Let's look at the current configuration:</p> <pre><code>    - name: container-restarts\n      successCondition: result[0] &gt; 0\n      failureLimit: 0\n      inconclusiveLimit: 0\n      consecutiveErrorLimit: 0\n      count: 1\n      provider:\n        prometheus:\n          address: http://prometheus-server.prometheus.svc.cluster.local\n          query: |\n            # There should be no restarts\n            sum(increase(kube_pod_container_status_restarts_total{\n              namespace=\"{{args.namespace}}\",\n              pod=~\"echo-server-.*\"\n            }[1m])) or vector(0)\n</code></pre> <p>The query checks for the number of container restarts in the last minute. If there are no restarts, the query returns 0. As the comment says, there should be no container restarts for success.</p> <p>But the success condition is set to <code>result[0] &gt; 0</code>, which means the metric only succeeds if there is at least one restart. That's the opposite of what we want! Let's change the success condition to <code>result[0] == 0</code>.</p> <p>After updating, commit and push your changes, then retry the rollout:</p> <pre><code>kubectl argo rollouts retry rollout echo-server -n echo-staging\n</code></pre> <p>This time, let's use the Argo Rollouts UI to check the status. Open the UI, select the <code>echo-staging</code> namespace in the top right, and click on the rollout card.</p> <p>After waiting a bit, we see the rollout still hasn't progressed and there are three analysis runs:</p> <p></p> <p>Clicking on <code>Analysis 2-1</code> shows the first error (wrong service), <code>Analysis 2-1.1</code> shows the previous error (wrong success condition), and <code>Analysis 2-1.2</code> is the latest error. Let's take a closer look:</p> <p></p> <p>Here, we see that the <code>container-restarts</code> metric was finally successful, but <code>ready-containers</code> is still failing. Let's click the metric on the left to investigate the query. It turns out it simply wasn't implemented yet:</p> <p></p> <p>Let's follow the comment and open the Prometheus UI to find a metric that helps us check the number of ready containers. By entering <code>kube_pod_container_status_</code>, autocompletion shows all available metrics. Let's choose <code>kube_pod_container_status_ready</code>.</p> <p></p> <p>Just like with the container-restarts metric, we want to filter for our namespace and pod. Let's add this and execute the query:</p> <pre><code>kube_pod_container_status_ready{\n  namespace=\"echo-staging\", \n  pod=~\"echo-server-.*\"\n}\n</code></pre> <p>This returns all ready containers for our pods:</p> <p></p> <p>But we don't want a list of all ready containers\u2014we want to check if there is at least one ready container. So, let's use the sum aggregator:</p> <pre><code>sum(\n  kube_pod_container_status_ready{\n    namespace=\"echo-staging\", \n    pod=~\"echo-server-.*\"\n  }\n)\n</code></pre> <p></p> <p>Nice! Now the query returns \"1\" instead of a list, which is exactly what we want.</p> <p>Before adding this to our manifest, let's remember we have two environments. Instead of hardcoding the namespace, let's use the same approach as in the other metric and use the <code>{{args.namespace}}</code> placeholder.</p> <p>The final query looks like this (with <code>or vector(0)</code> at the end to ensure the query returns 0 if no data is found):</p> <pre><code>query: |-\n  # Check how many containers are ready (should be at least 1)\n  sum(kube_pod_container_status_ready{\n    namespace=\"{{args.namespace}}\",\n    pod=~\"echo-server-.*\"\n  }) or vector(0)\n</code></pre> <p>Let's add this to our manifest, commit, push, refresh, and retry again.</p> <p>Yay! This time, the rollout actually progressed and the analysis runs are successful\u2014objective met! \ud83c\udf89</p>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#key-takeaways_1","title":"Key Takeaways","text":"<ul> <li>There are multiple effective ways to debug Argo Rollouts. Try them and use your favorite.</li> <li>Service references use the format <code>svc.namespace.svc.cluster.local</code>.</li> <li>Prometheus queries are a simple and effective way to validate application health during rollouts.</li> <li>The Prometheus UI is a great way to test and build your queries.</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#further-reading_1","title":"Further Reading","text":"<ul> <li>Argo Rollouts Analysis</li> <li>Prometheus Querying Basics</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#objective-3-two-working-promql-queries-in-the-analysistemplate-that-validate-application-health-during-releases","title":"Objective 3: Two working PromQL queries in the AnalysisTemplate that validate application health during releases","text":"<p>This objective is closely tied to Objective 2. By fixing both metrics in the AnalysisTemplate (<code>container-restarts</code> and <code>ready-containers</code>) we ensured that application health is properly validated during each rollout. Both queries now work as intended:</p> <ul> <li><code>container-restarts</code>: Confirms there are no container restarts during the rollout.</li> <li><code>ready-containers</code>: Checks that at least one container is ready before progressing.</li> </ul> <p>With these working PromQL queries, our rollouts are now protected by robust health checks. \ud83c\udf89</p>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#key-takeaways_2","title":"Key Takeaways","text":"<ul> <li>PromQL queries in the AnalysisTemplate provide automated, reliable health validation for deployments.</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#objective-4-all-rollouts-complete-successfully","title":"Objective 4: All rollouts complete successfully","text":"<p>This objective is also directly connected to Objective 2 and 3. After correcting the metrics in the AnalysisTemplate, the rollout was able to progress through all canary stages and complete successfully in both environments.</p> <p>With all objectives met, our deployment process is now fully automated and resilient!</p>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#complete-solution","title":"\u2705 Complete Solution","text":"<p>Here's what your corrected <code>AnalysisTemplate</code> should look like with all fixes applied:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: AnalysisTemplate\nmetadata:\n  name: echo-analysis\nspec:\n  args:\n    - name: namespace\n  metrics:\n    - name: container-restarts\n      successCondition: result[0] == 0\n      failureLimit: 0\n      inconclusiveLimit: 0\n      consecutiveErrorLimit: 0\n      count: 1\n      provider:\n        prometheus:\n          address: http://prometheus-server.prometheus.svc.cluster.local\n          query: |\n            # There should be no restarts\n            sum(increase(kube_pod_container_status_restarts_total{\n              namespace=\"{{args.namespace}}\",\n              pod=~\"echo-server-.*\"\n            }[1m])) or vector(0)\n    - name: ready-containers\n      successCondition: result[0] &gt;= 1\n      failureLimit: 0\n      inconclusiveLimit: 0\n      consecutiveErrorLimit: 0\n      count: 1\n      provider:\n        prometheus:\n          address: http://prometheus-server.prometheus.svc.cluster.local\n          query: |-\n            # Check how many containers are ready (should be at least 1)\n            sum(kube_pod_container_status_ready{\n              namespace=\"{{args.namespace}}\",\n              pod=~\"echo-server-.*\"\n            }) or vector(0)\n</code></pre> <p>With these changes, your rollouts will now progress automatically through canary stages based on health metrics, and all objectives of the challenge will be met. Great job! \ud83c\udf89</p>"}]}